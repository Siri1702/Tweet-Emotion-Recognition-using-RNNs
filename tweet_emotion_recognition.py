# -*- coding: utf-8 -*-
"""Tweet_Emotion_Recognition.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Crm4P_7d1SHqxX4P5SpwJDhBmBNpff30
"""

!pip install nlp

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline

import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
import nlp
import random

def show_history(h):
    epochs_trained = len(h.history['loss'])
    plt.figure(figsize=(16, 6))

    plt.subplot(1, 2, 1)
    plt.plot(range(0, epochs_trained), h.history.get('accuracy'), label='Training')
    plt.plot(range(0, epochs_trained), h.history.get('val_accuracy'), label='Validation')
    plt.ylim([0., 1.])
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.legend()

    plt.subplot(1, 2, 2)
    plt.plot(range(0, epochs_trained), h.history.get('loss'), label='Training')
    plt.plot(range(0, epochs_trained), h.history.get('val_loss'), label='Validation')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()
    plt.show()

def show_confusion_matrix(y_true, y_pred, classes):
    from sklearn.metrics import confusion_matrix

    cm = confusion_matrix(y_true, y_pred, normalize='true')

    plt.figure(figsize=(8, 8))
    sp = plt.subplot(1, 1, 1)
    ctx = sp.matshow(cm)
    plt.xticks(list(range(0, 6)), labels=classes)
    plt.yticks(list(range(0, 6)), labels=classes)
    plt.colorbar(ctx)
    plt.show()

from datasets import load_dataset

!pip install datasets

emotion_dataset = load_dataset("emotion")

emotion_dataset

train = emotion_dataset['train']
val = emotion_dataset['validation']
test = emotion_dataset['test']

def get_tweet(data):
  tweets = [x['text'] for x in data]
  labels = [x['label'] for x in data]
  return tweets,labels

tweets,labels = get_tweet(train)

tweets[0], labels[0]

from tensorflow.keras.preprocessing.text import Tokenizer

tokenizer = Tokenizer(num_words=10000,oov_token='<UNK>')
tokenizer.fit_on_texts(tweets)

tokenizer.texts_to_sequences([tweets[0]])

import matplotlib.pyplot as plt

"""Padding and Truncating as we need all of our inputs to be of the same shape"""

lengths = [len(t.split(' ')) for t in tweets]
plt.hist(lengths,bins = len(set(lengths)))
plt.show

max_len = 50
from tensorflow.keras.preprocessing.sequence import pad_sequences

def get_sequences(tokenizer,tweets):
  sequences = tokenizer.texts_to_sequences(tweets)
  padded = pad_sequences(sequences,truncating='post',padding='post',maxlen=max_len)
  return padded

padded_train_seq = get_sequences(tokenizer,tweets)

padded_train_seq[0]

classes = set(labels)

print(classes)

plt.hist(labels)

class_to_index = dict((c,i) for i,c in enumerate(classes))
index_to_class = dict((v,k) for k,v in class_to_index.items())

class_to_index

labels[0]

model = tf.keras.models.Sequential([
    tf.keras.layers.Embedding(10000,16,input_length = max_len),
    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(20,return_sequences=True)),
    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(20)),
    tf.keras.layers.Dense(6,activation = 'softmax')
])
model.compile(
    loss = 'sparse_categorical_crossentropy',
    optimizer='adam',
    metrics=['accuracy']
)

model.summary()

val_tweets,val_labels = get_tweet(val)
val_seq = get_sequences(tokenizer,val_tweets)

padded_train_seq = np.array(padded_train_seq)
labels = np.array(labels)
val_seq = np.array(val_seq)
val_labels = np.array(val_labels)
h = model.fit(
    padded_train_seq,labels,validation_data = (val_seq,val_labels),epochs=20,callbacks = [tf.keras.callbacks.EarlyStopping(monitor='val_accuracy',patience=2)]
)

show_history(h)

test_tweets,test_labels= get_tweet(test)
test_seq = get_sequences(tokenizer,test_tweets)

test_tweets =np.array(test_tweets)
test_labels = np.array(test_labels)

_=model.evaluate(test_seq,test_labels)

preds = (model.predict(test_seq) > 0.5).astype("int32")



